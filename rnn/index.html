<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>Explaining RNNs without neural networks</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="Explaining RNNs without neural networks"/>
<meta property='og:image' content="http://explained.ai/rnn/images/">
<meta property='og:description' content=""/>
<meta property='og:url' content="http://explained.ai/rnn/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="Explaining RNNs without neural networks">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="http://explained.ai/rnn/images/">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>Explaining RNNs without neural networks</h1>

<p><a href="http://parrt.cs.usfca.edu">Terence Parr</a><br><span style="font-size: 85%">Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a> and you might know him as the creator of the ANTLR parser generator.</span></p>

<p>
Vanilla recurrent neural networks (RNNs) form the basis of more sophisticated models, such as LSTMs and GRUs. There are lots of great articles, books, and videos that describe the functionality, mathematics, and behavior of RNNs, and I'm not trying to add yet another rehash here. (See below for a list of resources.) My goal is to present an explanation that avoids the neural network metaphor, stripping it down to its essence&mdash;a series of vector transformations that result in embeddings for variable-length input vectors.
</p>

<p>
My learning style involves pounding away at something until I'm able to re-create it myself from fundamental components. This helps me to understand exactly <b>what</b> a model is doing and <b>why</b> it is doing it.  You can ignore this article if you're familiar with  standard neural network layers and are comfortable with RNN explanations that use them as building blocks. Since I'm still learning the details of neural networks, I wanted to (1) peer through those layers to the matrices and vectors beneath and (2) investigate the details of the training process. My starting point was <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">Karpathy's RNN code snippet</a> associated with <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness">The Unreasonable Effectiveness of Recurrent Neural Networks</a> and then I absorbed minibatch details from Chapter 12 from Jeremy Howard's / Sylvan Gugger's book <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Deep Learning for Coders with fastai and PyTorch</a> and Chapter 12 from Andrew Trask's <a href="https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709">Grokking Deep Learning</a>.
</p>

<p><a href="images/vid-fast.gif">
<img align="right" src="images/vid-fast.gif" width="380" url="images/vid-fast.">
</a>
In this article, I hope to contribute a simple and visually-focused data-transformation perspective on RNNs using a trivial data set that maps words for "cat" to the associated natural language.   The animation on the right was taken (and speeded up) from a <a href="https://youtu.be/ppz0XdEcGF4">youtube clip</a> I made for this article. For my actual PyTorch-based implementations, I've provided notebooks that use a nontrivial <a href="https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz">family name to natural language</a> data set:
</p>

<ul>
	<li><a href="https://github.com/parrt/ml-articles/blob/master/rnn/notebooks/SGD.ipynb">SGD</a> (parameters updated after each record)
	<li><a href="https://github.com/parrt/ml-articles/blob/master/rnn/notebooks/minibatch.ipynb">minibatch</a> (parameters updated after a small batch of records)
	<li><a href="https://github.com/parrt/ml-articles/blob/master/rnn/notebooks/vectorized.ipynb">vectorized minibatch</a> (convert for-loop into matrix multiply)
	<li><a href="https://github.com/parrt/ml-articles/blob/master/rnn/notebooks/gpu.ipynb">vectorized minibatch running on a GPU</a> (use PyTorch to compute on GPU)
</ul>

<h2>TL;DR &nbsp;Details I learned </h2>

<p></p>

For those readers who already have a good understanding of RNN concepts, let me summarize some of the key implementation bits that I learned while coding these examples:

<p><b>still working on this..</b>

<ul>
<li>when do we update parameters?
<li> what is h?
	<li>  Is V part of the RNN?
		<li>relu  explodes unless we constrain  model parameters. use tanh
	<li>minibatching between records and within records
	<li>`h` is initialized to zero for each word.
	<li>`h` is not updated as part of the gradient descent process; `h` is a temporary variable holding partial results, not part of the model.
	<li> must pad on the right not the left
	<li> bptt have anything to do with this? no
	<li>Matrices $W$, $U$, $V$ are initialized exactly once before training begins.
	<li>$W$, $U$, $V$ are updated as part of the gradient descent process but only once per word. That is, they are updated after the  dense vector `h` has been computed for each word.
	<li>As we iterate through the characters of a single word, the $W$, $U$, and $V$ matrices do not change.
</ul>

<h2>Table of contents</h2>

If you're like me, you like to see things laid out explicitly so you don't have to think too much ðŸ˜€.   In the two main pages outlined below, I've tried to lay out all of the details I've learned, using as many pictures as I could.

<ul>
<li><a href="implementation.html">1. An RNN built with matrices and trained with SGD</a>
<ul>
	<li><a href="implementation.html#sec:1.1">The goal: meaningful vectors representing words</a>
	</li>
	<li><a href="implementation.html#sec:1.2">Encoding words as integers</a>
	</li>
	<li><a href="implementation.html#sec:1.3">Aggregating character vectors to encode words</a>
	</li>
	<li><a href="implementation.html#sec:1.4">Encoding and aggregating character vectors through matrix transforms</a>
	</li>
	<li><a href="implementation.html#sec:1.5">Learning the RNN matrices by training a classifier</a>
	</li>
</ul>

<li>
	<a href="minibatch.html">2 Training an RNN with vectorized minibatch SGD</a>
<ul>
	<li><a href="minibatch.html#sec:2.1">Computing loss using more than one input record</a>
	</li>
	<li><a href="minibatch.html#sec:2.2">Converting the batch loop to vector operations</a>
	<ul>
		<li><a href="minibatch.html#sec:2.2.1">Processing batches, one character position at a time</a></li>
		<li><a href="minibatch.html#sec:2.2.2">Padding short words with 0 vectors on the left</a></li>
	</ul>
	</li>
</ul>	

</ul>

<!--
><a href="images/equation-vec-W-U-V.svg">
		<img align="right" src="images/equation-vec-W-U-V.svg" width="30%" url="images/equation-vec-W-U-V.svg">
		</a>
<a href="images/3D-matrix-right.png">
<img align="right" src="images/3D-matrix-right.png" width="140" url="images/3D-matrix-right.png">
</a>
-->

<h2>Resources</h2>

First off, if you are new to deep learning, check out Jeremy Howard's full course (with video lectures) called <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a>.

As for recurrent neural networks in particular, here are a few resources that I found  useful:
 
<ul>
<li>To get started with RNNs, I think the best first stop is probably <a href="https://www.youtube.com/watch?v=SEnXr6v2ifU">MIT's RNN intro video</a> to get an overview.

<li>Karpathy's well-known <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">RNN blog</a> and associated <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">code snippet</a> are really motivating and I used them as a basis to understand how the vectors flow through the RNN recurrence relation.

<li>Chapter 12 from Jeremy Howard's and Sylvan Gugger's book <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Deep Learning for Coders with fastai and PyTorch</a> is chock-full of both high-level and low-level details. The other chapters are well worth reading too.
	
<li>Chapters 11 and 12 from Andrew Trask's <a href="https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709">Grokking Deep Learning</a> has a lot of good stuff on word vectors and RNNs.

<li>Section 6.2 in FranÃ§ois Chollet's <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a> book does a really nice discussion and implementation using neural network layers; the book is very clearly written. It does, however, focus on keras whereas I'm using PyTorch for tensors in this article.

<li>Yannet Interian has a good <a href="https://github.com/yanneta/dl-course/blob/master/rnn-name2lang.ipynb">notebook for training RNNs that map family names to languages</a>.
</ul>

<h2>Acknowledgements</h2>

I'd like to thank Yannet Interian, also faculty in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>, for acting as a resource and pointing me to relevant material.  <a href="https://twitter.com/bearpelican">Andrew Shaw</a> and <a href="https://zeigermann.eu">Oliver Zeigermann</a> also answered a lot of my questions and filled in lots of implementation details.

</body>
</html>
