<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>Explaining RNNs without neural networks</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="Explaining RNNs without neural networks"/>
<meta property='og:image' content="http://explained.ai/rnn/images/vid-fast.gif">
<meta property='og:description' content="This article explains how recurrent neural networks (RNN's) work without using the neural network metaphor. It uses a visually-focused data-transformation perspective to show how RNNs encode variable-length input vectors as fixed-length embeddings. Included are PyTorch implementation notebooks that use just linear algebra and the autograd feature."/>
<meta property='og:url' content="http://explained.ai/rnn/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="Explaining RNNs without neural networks">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="This article explains how recurrent neural networks (RNN's) work without using the neural network metaphor. It uses a visually-focused data-transformation perspective to show how RNNs encode variable-length input vectors as fixed-length embeddings. Included are PyTorch implementation notebooks that use just linear algebra and the autograd feature.">
<meta name="twitter:image" content="http://explained.ai/rnn/images/vid-fast.gif">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>Explaining RNNs without neural networks</h1>

<p style="line-height:1.1;"><a href="http://parrt.cs.usfca.edu">Terence Parr</a><br><span style="font-size: 85%; line-height:1.1;">Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a> and you might know him as the creator of the ANTLR parser generator.</span></p>

<p>
Vanilla recurrent neural networks (RNNs) form the basis of more sophisticated models, such as LSTMs and GRUs. There are lots of great articles, books, and videos that describe the functionality, mathematics, and behavior of RNNs so, don't worry, this isn't yet another rehash. (See below for a list of resources.) My goal is to present an explanation that avoids the neural network metaphor, stripping it down to its essence&mdash;a series of vector transformations that result in embeddings for variable-length input vectors.
</p>

<p>
My learning style involves pounding away at something until I'm able to re-create it myself from fundamental components. This helps me to understand exactly <b>what</b> a model is doing and <b>why</b> it is doing it.  You can ignore this article if you're familiar with  standard neural network layers and are comfortable with RNN explanations that use them as building blocks. Since I'm still learning the details of neural networks, I wanted to (1) peer through those layers to the matrices and vectors beneath and (2) investigate the details of the training process. My starting point was <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">Karpathy's RNN code snippet</a> associated with <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness">The Unreasonable Effectiveness of Recurrent Neural Networks</a> and then I absorbed details from Chapter 12 from Jeremy Howard's / Sylvain Gugger's book <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Deep Learning for Coders with fastai and PyTorch</a> and Chapter 12 from Andrew Trask's <a href="https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709">Grokking Deep Learning</a>.
</p>

<p><a href="images/vid-fast.gif">
<img align="right" src="images/vid-fast.gif" width="380" url="images/vid-fast.">
</a>
In this article, I hope to contribute a simple and visually-focused data-transformation perspective on RNNs using a trivial data set that maps words for "cat" to the associated natural language.   The animation on the right was taken (and speeded up) from a <a href="https://youtu.be/ppz0XdEcGF4">youtube clip</a> I made for this article. For my actual PyTorch-based implementations, I've provided notebooks that use a nontrivial <a href="https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz">family name to natural language</a> data set. These links open my full implementation notebooks at colab:
</p>

<ul>
	<li><a href="https://colab.research.google.com/github/parrt/ml-articles/blob/master/rnn/notebooks/SGD.ipynb">SGD</a> (parameters updated after each record)
	<li><a href="https://colab.research.google.com/github/parrt/ml-articles/blob/master/rnn/notebooks/minibatch.ipynb">minibatch</a> (parameters updated after a small batch of records)
	<li><a href="https://colab.research.google.com/github/parrt/ml-articles/blob/master/rnn/notebooks/vectorized.ipynb">vectorized minibatch</a> (convert for-loop into matrix multiply)
	<li><a href="https://colab.research.google.com/github/parrt/ml-articles/blob/master/rnn/notebooks/gpu.ipynb">vectorized minibatch running on a GPU</a> (use PyTorch to compute on GPU)
</ul>

<h2>TL;DR &nbsp;Details I learned </h2>

<p></p>

As I tried to learn RNNs, my brain kept wondering about the implementation details and key concepts, such as what exactly was contained in the hidden state vector.  My brain appears to be so literal that it can't understand anything until it sees the entire picture in depth.   For those in a hurry, let me summarize some of the key things I learned by implementing RNNs with nothing but matrices and vectors.  The full table of contents for the full article appears below.

<ul>
	<li>What exactly is <i>h</i> (sometimes called <i>s</i>) in the recurrence relation representing an RNN: <img src="images/blkeqn-F224DE1728AA3CC4724E5AE9C5429D65.svg"  style="vertical-align: -3pt;"> (leaving off the nonlinearity)? The variable name <i>h</i> is typically used because it represents the <b>h</b>idden state of the RNN. An RNN takes a variable-length input record of symbols (e.g., stock price sequence, document, sentence, or word) and generates a fixed-length vector in high dimensional space, called an embedding, that somehow meaningfully represents or encodes the input record. The vector is only associated with a single input record and is only meaningful in the context of a classification or regression problem; the RNN is just a component of a surrounding model.  For example, the <i>h</i> vector is often passed through a final linear layer <i>V</i> (multiclass logistic regressor) to get model predictions. 	</li>
	<li>Does <i>h</i> contain learned parameters of the model? No. Vector <i>h</i> is a local variable holding the partial result as we process symbols of a single record but becomes the final embedding vector after the RNN processes the final input symbol. This vector is not updated as part of the gradient descent process; it is computed using the recurrence relation given above.</li>
	<li>Is <i>h</i> the RNN output?  I think it depends on your perspective. Yes, that embedding vector comes out of the RNN and becomes the input to following layers, but it's definitely not the output of the entire model. The model output comes from, say, the application of another matrix, <i>V</i> to <i>h</i>.</li>
	<li>What is <i>t</i> and does it represent time? If your variable-length input record is a timeseries like sensor or stock quote data, then yes <i>t</i> represents time. Variable <i>t</i> is really just the iterator variable used by the RNN to step through the symbols of a single input record.</li>
	<li>What is <i>backpropagation through time</i> (BPTT)? BPTT is stochastic gradient descent (SGD) as applied to the specific case of RNNs that often process timeseries data. Backpropagation by itself means updating the parameters of the model in the direction of lower loss. BPTT refers to the case where we perform BP on <i>m</i> layers that reuse the same <i>W</i> and <i>U</i> for <i>m</i> symbols in the input record.</li>
	<li>Then what's <i>truncated backpropagation</i> or truncated BPTT? (First, let me point out that we don't need truncated BPTT for fairly short input records, such as we have for family names; my examples do not need to worry about truncated BPTT.) For large input records, such as documents, gradients across all (unrolled) RNN layers become expensive to compute and tend to vanish or explode, depending on our nonlinear activation function. To overcome this problem, we can simply stop the BP process after a certain number of gradient computations in the computation graph. It means not being able to update the model parameters based upon input symbols much earlier in the input stream.  I sometimes see the length of the truncated window represented with variable <tt>bptt</tt> in code, which is pretty confusing. <b>Note</b> that <i>h</i> is still computed using the full computation as described by the recurrence relation.  Truncated BP simply refers to how much information we use from BP to update the parameter models in <i>W</i> and <i>U</i> (and usually <i>V</i>). Vector <i>h</i> uses <i>W</i> and <i>U</i> but is not updated by BP. Model <tt>LMModel3</tt> and Section "Maintaining the State of an RNN" of <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Chapter 12 in the fastai book</a> explain this in detail.</li>
	<li>Each variable <i>h</i> is associated with a single input record and is initialized to the zero vector at the start of the associated record.</li>
	<li>Matrices <i>W</i>, <i>U</i>, <i>V</i> are <b>initialized</b> exactly once: before training begins.</li>
	<li>Matrices <i>W</i>, <i>U</i>, <i>V</i> are <b>updated</b> as part of the SGD process after the <i>h</i> embedding vector has been computed for each input record in the batch (or single word if using pure SGD). As we iterate through the symbols in time, the <i>W</i>, <i>U</i>, <i>V</i> matrices do not change, unless we are using truncated BPTT for very long input records.</li>
	<li>Minibatching is a small subset of the input records split between records, leaving all input records intact.  However, in the situation where the input records are very big, minibatching can even involve splitting individual records, rather than just between records. Each record in a minibatch requires a separate <i>h</i> vector, leading to matrix <i>H</i> in my examples.</li>
	<li>When combining one-hot vectors for minibatching purposes, we must pad on the left not the right to avoid changing the computation. See this section: <a href="minibatch.html#sec:2.2.2">Padding short words with 0 vectors on the left</a>.</li>
</ul>

<h2>Table of contents</h2>

I've broken up this article into two main sections. The first section tries to identify how an RNN encodes a variable-length input record as a fixed-length vector by reinventing the mechanism in baby steps.   The second section is all about minibatching details and vectorizing the gradient descent training loop.

<ul>
<li><a href="implementation.html">1. An RNN built with matrices and trained with SGD</a>
<ul>
	<li><a href="implementation.html#sec:1.1">The goal: meaningful vectors representing words</a>
	</li>
	<li><a href="implementation.html#sec:1.2">Encoding words as integers</a>
	</li>
	<li><a href="implementation.html#sec:1.3">Aggregating character vectors to encode words</a>
	</li>
	<li><a href="implementation.html#sec:1.4">Encoding and aggregating character vectors through matrix transforms</a>
	</li>
	<li><a href="implementation.html#sec:1.5">Learning the RNN matrices by training a classifier</a>
	</li>
</ul>

<li>
	<a href="minibatch.html">2 Training an RNN with vectorized minibatch SGD</a>
<ul>
	<li><a href="minibatch.html#sec:2.1">Computing loss using more than one input record</a>
	</li>
	<li><a href="minibatch.html#sec:2.2">Converting the batch loop to vector operations</a>
	<ul>
		<li><a href="minibatch.html#sec:2.2.1">Processing batches, one character position at a time</a></li>
		<li><a href="minibatch.html#sec:2.2.2">Padding short words with 0 vectors on the left</a></li>
	</ul>
	</li>
</ul>	

</ul>

<!--
><a href="images/equation-vec-W-U-V.svg">
		<img align="right" src="images/equation-vec-W-U-V.svg" width="30%" url="images/equation-vec-W-U-V.svg">
		</a>
<a href="images/3D-matrix-right.png">
<img align="right" src="images/3D-matrix-right.png" width="140" url="images/3D-matrix-right.png">
</a>
-->

<h2>Resources</h2>

First off, if you are new to deep learning, check out Jeremy Howard's full course (with video lectures) called <a href="https://course.fast.ai/">Practical Deep Learning for Coders</a>.

As for recurrent neural networks in particular, here are a few resources that I found  useful:
 
<ul>
<li>To get started with RNNs, I think the best first stop is probably <a href="https://www.youtube.com/watch?v=SEnXr6v2ifU">MIT's RNN intro video</a> to get an overview.

<li>Karpathy's well-known <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">RNN blog</a> and associated <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">code snippet</a> are really motivating and I used them as a basis to understand how the vectors flow through the RNN recurrence relation.

<li>Chapter 12 from Jeremy Howard's and Sylvain Gugger's book <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Deep Learning for Coders with fastai and PyTorch</a> is chock-full of both high-level and low-level details. The other chapters are well worth reading too.
	
<li>Chapters 11 and 12 from Andrew Trask's <a href="https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709">Grokking Deep Learning</a> has a lot of good stuff on word vectors and RNNs.

<li>Section 6.2 in François Chollet's <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a> book does a really nice discussion and implementation using neural network layers; the book is very clearly written. It does, however, focus on keras whereas I'm using PyTorch for tensors in this article.

<li>Yannet Interian has a good <a href="https://github.com/yanneta/dl-course/blob/master/rnn-name2lang.ipynb">notebook for training RNNs that map family names to languages</a>.
</ul>

<h2>Acknowledgements</h2>

I'd like to thank Yannet Interian, also faculty in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>, for acting as a resource and pointing me to relevant material.  <a href="https://twitter.com/bearpelican">Andrew Shaw</a> and <a href="https://zeigermann.eu">Oliver Zeigermann</a> also answered a lot of my questions and filled in lots of implementation details.

</body>
</html>
