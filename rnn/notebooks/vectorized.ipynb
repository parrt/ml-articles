{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using vectorized mini-batch SGD\n",
    "\n",
    "This notebook is part of article [Explaining RNNs without neural networks](https://explained.ai/rnn/index.html) and notebook [prep.ipynb](prep.ipynb) should be run before this notebook as it needs files: `data/X.pkl` and `data/y.pkl`.\n",
    "\n",
    "Instead of processing batch one record at a time from time 1 to time len(word), process all time steps t across all batch records at once, then proceed to time step (char index) t+1.  This allows us to vectorize and perform each time step in parallel.  We effectively remove a loop.\n",
    "\n",
    "But, it means we must pad to have same length in batch. We pad on left so the zero vectors are ignored to get same answer as record-by-record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "!if ! test -f support.py; then wget https://raw.githubusercontent.com/parrt/ml-articles/master/rnn/notebooks/support.py; fi\n",
    "\n",
    "from support import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TESTING SUBSAMPLE\n",
    "idx = list(np.random.randint(0,len(X),size=2000))\n",
    "X = np.array(X)[idx].tolist()\n",
    "y = np.array(y)[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_matrix(X, max_len, vocab, verbose=False):\n",
    "    X_onehot = torch.zeros(len(X),max_len,len(vocab), dtype=torch.float64)\n",
    "    for i,x in enumerate(X):\n",
    "        pad = max_len - len(x)\n",
    "        for j,c in enumerate(x):\n",
    "            X_onehot[i, j+pad, ctoi[c]] = 1\n",
    "        if verbose: print(x); print(X_onehot[i].T, \"\\n\")\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X:Sequence[Sequence], max_len:int, vocab:dict):\n",
    "    \"Cut-n-paste from body of training for use with metrics\"\n",
    "    X_onehot = onehot_matrix(X, max_len, vocab)\n",
    "    h = torch.zeros(nhidden, len(X), dtype=torch.float64, requires_grad=False)\n",
    "    for j in range(max_len):\n",
    "        x_step_t = X_onehot[:,j].T\n",
    "        h = W.mm(h) + U.mm(x_step_t)\n",
    "        h = torch.tanh(h)        \n",
    "    o = V.mm(h)\n",
    "    o = o.T # make it batch_size x nclasses\n",
    "    o = softmax(o)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Just some matrices. First, set up hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with trivial data set\n",
    "\n",
    "Set TESTING=True to test vs full X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "\n",
    "nhidden = 100\n",
    "batch_size = 32\n",
    "\n",
    "if TESTING:\n",
    "    nhidden = 2\n",
    "    batch_size = 2\n",
    "\n",
    "    X_train = [['a','b'],['c','d','e'], # batch 1\n",
    "               ['f'],['c','a'], # batch 2\n",
    "               ['e']] # strip\n",
    "    y_train = [0,2,1,1,2]\n",
    "\n",
    "    X_valid = X_train\n",
    "    y_valid = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,688 training records, batch size 32, 29 features (chars), 18 target languages, state is 100-vector\n"
     ]
    }
   ],
   "source": [
    "n = len(X_train)\n",
    "\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X_train = X_train[0:n]\n",
    "y_train = y_train[0:n]\n",
    "vocab, ctoi = getvocab(X_train)\n",
    "max_len = get_max_len(X_train)\n",
    "nfeatures = len(vocab)\n",
    "nclasses = len(torch.unique(y_train))\n",
    "\n",
    "print(f\"{n:,d} training records, batch size {batch_size}, {nfeatures} features (chars), {nclasses} target languages, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['n', 'a', 'j', 'j', 'a', 'r'],\n",
       " ['z', 'h', 'e', 'l', 'o', 'k', 'h', 'o', 'v', 't', 's', 'e', 'v']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_onehot = onehot_matrix(X_train, max_len, vocab, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With verbose and trivial X_train we get:\n",
    "\n",
    "```\n",
    "tensor([[[0., 0., 0., 0., 0., 0.],\n",
    "         [1., 0., 0., 0., 0., 0.],\n",
    "         [0., 1., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 1., 0., 0., 0.],\n",
    "         [0., 0., 0., 1., 0., 0.],\n",
    "         [0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 1., 0., 0., 0.],\n",
    "         [1., 0., 0., 0., 0., 0.]]], dtype=torch.float64)\n",
    "```\n",
    "\n",
    "With `X_onehot.shape` = [4, 3, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using vectorized minibatch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 accum loss  2.0299 accur 0.590 | train loss  1.2916 accur 0.646 | valid loss  1.4388 accur 0.636\n",
      "Epoch:   2 accum loss  1.1531 accur 0.677 | train loss  1.0566 accur 0.697 | valid loss  1.2115 accur 0.674\n",
      "Epoch:   3 accum loss  1.0215 accur 0.709 | train loss  0.9183 accur 0.733 | valid loss  1.1167 accur 0.696\n",
      "Epoch:   4 accum loss  0.9407 accur 0.728 | train loss  0.9085 accur 0.732 | valid loss  1.1172 accur 0.695\n",
      "Epoch:   5 accum loss  0.8937 accur 0.742 | train loss  0.8424 accur 0.753 | valid loss  1.0622 accur 0.721\n",
      "Epoch:   6 accum loss  0.8501 accur 0.747 | train loss  0.8273 accur 0.756 | valid loss  1.0720 accur 0.718\n",
      "Epoch:   7 accum loss  0.8195 accur 0.755 | train loss  0.8094 accur 0.767 | valid loss  1.0763 accur 0.719\n",
      "Epoch:   8 accum loss  0.8157 accur 0.756 | train loss  0.7213 accur 0.776 | valid loss  0.9837 accur 0.738\n",
      "Epoch:   9 accum loss  0.7773 accur 0.765 | train loss  0.7274 accur 0.781 | valid loss  1.0054 accur 0.739\n",
      "Epoch:  10 accum loss  0.7524 accur 0.775 | train loss  0.8057 accur 0.766 | valid loss  1.1054 accur 0.716\n",
      "Epoch:  11 accum loss  0.7362 accur 0.780 | train loss  0.7607 accur 0.768 | valid loss  1.0258 accur 0.723\n",
      "Epoch:  12 accum loss  0.7240 accur 0.783 | train loss  0.7031 accur 0.788 | valid loss  0.9825 accur 0.735\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 s, sys: 1.27 s, total: 29.3 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.005, weight_decay=0.0)\n",
    "\n",
    "history = []\n",
    "epochs = 12\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total = 0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        batch_X = X_train[p:p+batch_size]\n",
    "        batch_y = y_train[p:p+batch_size]\n",
    "        batch_X_onehot = onehot_matrix(batch_X, max_len, vocab)\n",
    "        H = torch.zeros(nhidden, batch_size, dtype=torch.float64, requires_grad=False)\n",
    "        for t in range(max_len):\n",
    "            x_step_t = batch_X_onehot[:,t].T # make it len(vocab) x batch_size\n",
    "            H = W.mm(H) + U.mm(x_step_t)\n",
    "            H = torch.tanh(H)\n",
    "        o = V.mm(H)\n",
    "        o = o.T # make it batch_size x nclasses\n",
    "        o = softmax(o)\n",
    "        loss = cross_entropy(o, batch_y)\n",
    "#         print(loss.item())\n",
    "        correct = torch.argmax(o, dim=1)==batch_y\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        total += len(batch_y)\n",
    "\n",
    "        # update matrices based upon loss computed from a batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    epoch_training_loss /= nbatches\n",
    "    epoch_training_accur /= n\n",
    "#     print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        o = forward(X_train, max_len, vocab)#, apply_softmax=False)\n",
    "        train_loss = cross_entropy(o, y_train).item()\n",
    "        correct = torch.argmax(o, dim=1).detach()==y_train\n",
    "        train_accur = torch.sum(correct) / float(len(X_train))\n",
    "\n",
    "        o = forward(X_valid, max_len, vocab)\n",
    "        valid_loss = cross_entropy(o, y_valid).item()\n",
    "        correct = torch.argmax(o, dim=1).detach()==y_valid\n",
    "        valid_accur = torch.sum(correct) / float(len(X_valid))\n",
    "\n",
    "        history.append((train_loss, valid_loss))\n",
    "        print(f\"Epoch: {epoch:3d} accum loss {epoch_training_loss:7.4f} accur {epoch_training_accur:4.3f} | train loss {train_loss:7.4f} accur {train_accur:4.3f} | valid loss {valid_loss:7.4f} accur {valid_accur:4.3f}\")\n",
    "\n",
    "history = torch.tensor(history)\n",
    "plot_history(history, yrange=(0,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing on 80% training from full data set:\n",
    "\n",
    "```\n",
    "CPU times: user 40.3 s, sys: 749 ms, total: 41 s\n",
    "Wall time: 41 s\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
