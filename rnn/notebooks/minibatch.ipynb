{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using mini-batch SGD, multiple records used to compute gradient\n",
    "\n",
    "This notebook is part of article [Explaining RNNs without neural networks](https://explained.ai/rnn/index.html) and notebook [prep.ipynb](prep.ipynb) should be run before this notebook as it needs files: `data/X.pkl` and `data/y.pkl`.\n",
    "\n",
    "Still w/o vectorization, we train one full record at a time; we just do a batch of words before computing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "!if ! test -f support.py; then wget https://raw.githubusercontent.com/parrt/ml-articles/master/rnn/notebooks/support.py; fi\n",
    "\n",
    "from support import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TESTING SUBSAMPLE\n",
    "idx = list(np.random.randint(0,len(X),size=2000))\n",
    "X = np.array(X)[idx].tolist()\n",
    "y = np.array(y)[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward1(x):\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "    for j in range(len(x)):  # for each char in a name\n",
    "        x_onehot = onehot(x[j])\n",
    "        h = W.mm(h) + U.mm(x_onehot)# + b\n",
    "        h = torch.tanh(h)\n",
    "    # h is output of RNN, a fancy CBOW embedding for variable-length sequence in x\n",
    "    # run through a final layer to map that h to a one-hot encoded predicted class\n",
    "    o = V.mm(h)# + Vb\n",
    "    o = o.reshape(1,nclasses)\n",
    "    o = softmax(o)\n",
    "    return o\n",
    "\n",
    "def forward(X:Sequence[Sequence]):#, apply_softmax=True):\n",
    "    \"Cut-n-paste from body of training for use with metrics\"\n",
    "    outputs = []\n",
    "    for i in range(0, len(X)): # for each input record\n",
    "        o = forward1(X[i])\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Just some matrices. First, set up hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(c) -> torch.tensor:\n",
    "    v = torch.zeros((len(vocab),1), dtype=torch.float64)\n",
    "    v[ctoi[c]] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,688 training records, 29 features (chars), 18 target languages, state is 100-vector\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nhidden = 100\n",
    "\n",
    "n = len(X_train)\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X_train = X_train[0:n]\n",
    "y_train = y_train[0:n]\n",
    "\n",
    "nfeatures = len(vocab)\n",
    "nclasses = len(torch.unique(y_train))\n",
    "\n",
    "print(f\"{n:,d} training records, {nfeatures} features (chars), {nclasses} target languages, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using minibatch SGD, multiple records used to compute gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 accum loss  2.2988 accur 0.595 | train loss  1.3340 accur 0.690 | valid loss  1.5054 accur 0.673\n",
      "Epoch:   2 accum loss  1.2110 accur 0.692 | train loss  1.1126 accur 0.713 | valid loss  1.3643 accur 0.693\n",
      "Epoch:   3 accum loss  1.0127 accur 0.732 | train loss  0.9208 accur 0.748 | valid loss  1.2055 accur 0.719\n",
      "Epoch:   4 accum loss  0.9369 accur 0.738 | train loss  0.8291 accur 0.760 | valid loss  1.1878 accur 0.712\n",
      "Epoch:   5 accum loss  0.8820 accur 0.751 | train loss  0.8583 accur 0.771 | valid loss  1.2547 accur 0.718\n",
      "Epoch:   6 accum loss  0.7924 accur 0.772 | train loss  0.8025 accur 0.783 | valid loss  1.2016 accur 0.733\n",
      "Epoch:   7 accum loss  0.7573 accur 0.778 | train loss  0.6809 accur 0.796 | valid loss  1.1164 accur 0.722\n",
      "Epoch:   8 accum loss  0.7496 accur 0.774 | train loss  0.6266 accur 0.814 | valid loss  1.0421 accur 0.749\n",
      "Epoch:   9 accum loss  0.6863 accur 0.794 | train loss  0.6297 accur 0.815 | valid loss  1.1172 accur 0.743\n",
      "Epoch:  10 accum loss  0.6525 accur 0.802 | train loss  0.5793 accur 0.823 | valid loss  1.0839 accur 0.740\n",
      "Epoch:  11 accum loss  0.6607 accur 0.800 | train loss  0.6278 accur 0.803 | valid loss  1.1605 accur 0.719\n",
      "Epoch:  12 accum loss  0.6602 accur 0.798 | train loss  0.6176 accur 0.817 | valid loss  1.1511 accur 0.739\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 30s, sys: 192 ms, total: 3min 30s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.005, weight_decay=0.0)\n",
    "\n",
    "history = []\n",
    "epochs = 12\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        for i in range(p, p+batch_size): # do one batch\n",
    "            x = X_train[i]\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(x)):  # for each char in a name\n",
    "                h = W.mm(h) + U.mm(onehot(x[j]))\n",
    "                h = torch.tanh(h)\n",
    "            # h is output of RNN, a fancy CBOW embedding for variable-length sequence in x\n",
    "            # run through a final layer to map that h to a one-hot encoded predicted class\n",
    "#             h = dropout(h, p=0.3)\n",
    "            o = V.mm(h)\n",
    "            o = o.reshape(1,nclasses)\n",
    "            o = softmax(o)\n",
    "            loss += cross_entropy(o, y_train[i])\n",
    "            correct = torch.argmax(o[0])==y_train[i]\n",
    "            epoch_training_accur += correct\n",
    "\n",
    "        # update matrices based upon loss computed from a batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "#         print(loss.detach().item())\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    epoch_training_loss /= n\n",
    "    epoch_training_accur /= n\n",
    "#     print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        o = forward(X_train)#, apply_softmax=False)\n",
    "        train_loss = cross_entropy(o, y_train)\n",
    "        correct = torch.argmax(o, dim=1).detach()==y_train\n",
    "        train_accur = torch.sum(correct) / float(len(X_train))\n",
    "\n",
    "        o = forward(X_valid)\n",
    "        valid_loss = cross_entropy(o, y_valid)\n",
    "        correct = torch.argmax(o, dim=1).detach()==y_valid\n",
    "        valid_accur = torch.sum(correct) / float(len(X_valid))\n",
    "\n",
    "        history.append((train_loss, valid_loss))\n",
    "        print(f\"Epoch: {epoch:3d} accum loss {epoch_training_loss:7.4f} accur {epoch_training_accur:4.3f} | train loss {train_loss:7.4f} accur {train_accur:4.3f} | valid loss {valid_loss:7.4f} accur {valid_accur:4.3f}\")\n",
    "\n",
    "history = torch.tensor(history)\n",
    "plot_history(history, yrange=(0,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing on 80% training from full data set:\n",
    "\n",
    "```\n",
    "CPU times: user 3min 30s, sys: 192 ms, total: 3min 30s\n",
    "Wall time: 3min 30s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
