{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using mini-batch SGD, multiple records used to compute gradient\n",
    "\n",
    "This notebook is part of article [Explaining RNNs without neural networks](https://explained.ai/rnn/index.html) and notebook [prep.ipynb](prep.ipynb) should be run before this notebook as it needs files: `data/X.pkl` and `data/y.pkl`.\n",
    "\n",
    "Still w/o vectorization, we train one full record at a time; we just do a batch of words before computing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "!if ! test -f support.py; then wget https://raw.githubusercontent.com/parrt/ml-articles/master/rnn/notebooks/support.py; fi\n",
    "\n",
    "from support import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TESTING SUBSAMPLE\n",
    "idx = list(np.random.randint(0,len(X),size=2000))\n",
    "X = np.array(X)[idx].tolist()\n",
    "y = np.array(y)[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward1(x):\n",
    "    h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "    for j in range(len(x)):  # for each char in a name\n",
    "        x_onehot = onehot(x[j])\n",
    "        h = W.mm(h) + U.mm(x_onehot)# + b\n",
    "        h = torch.tanh(h)\n",
    "    # h is output of RNN, a fancy CBOW embedding for variable-length sequence in x\n",
    "    # run through a final layer to map that h to a one-hot encoded predicted class\n",
    "    o = V.mm(h)# + Vb\n",
    "    o = o.reshape(1,nclasses)\n",
    "    o = softmax(o)\n",
    "    return o\n",
    "\n",
    "def forward(X:Sequence[Sequence]):#, apply_softmax=True):\n",
    "    \"Cut-n-paste from body of training for use with metrics\"\n",
    "    outputs = []\n",
    "    for i in range(0, len(X)): # for each input record\n",
    "        o = forward1(X[i])\n",
    "        outputs.append( o[0] ) \n",
    "    return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Just some matrices. First, set up hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(c) -> torch.tensor:\n",
    "    v = torch.zeros((len(vocab),1), dtype=torch.float64)\n",
    "    v[ctoi[c]] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,688 training records, 29 features (chars), 18 target languages, state is 100-vector\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nhidden = 100\n",
    "\n",
    "n = len(X_train)\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X_train = X_train[0:n]\n",
    "y_train = y_train[0:n]\n",
    "\n",
    "nfeatures = len(vocab)\n",
    "nclasses = len(torch.unique(y_train))\n",
    "\n",
    "print(f\"{n:,d} training records, {nfeatures} features (chars), {nclasses} target languages, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using minibatch SGD, multiple records used to compute gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 accum loss  2.1903 accur 0.610 | train loss  1.2386 accur 0.701 | valid loss  1.3875 accur 0.675\n",
      "Epoch:   2 accum loss  1.1993 accur 0.698 | train loss  0.9513 accur 0.744 | valid loss  1.1545 accur 0.714\n",
      "Epoch:   3 accum loss  1.0053 accur 0.733 | train loss  0.8485 accur 0.768 | valid loss  1.0977 accur 0.729\n",
      "Epoch:   4 accum loss  0.8751 accur 0.756 | train loss  0.7564 accur 0.783 | valid loss  1.0264 accur 0.738\n",
      "Epoch:   5 accum loss  0.8067 accur 0.770 | train loss  0.7307 accur 0.787 | valid loss  1.0335 accur 0.743\n",
      "Epoch:   6 accum loss  0.7458 accur 0.780 | train loss  0.6762 accur 0.792 | valid loss  1.0246 accur 0.736\n",
      "Epoch:   7 accum loss  0.7074 accur 0.791 | train loss  0.7153 accur 0.788 | valid loss  1.0633 accur 0.744\n",
      "Epoch:   8 accum loss  0.7015 accur 0.792 | train loss  0.7343 accur 0.784 | valid loss  1.1038 accur 0.727\n",
      "Epoch:   9 accum loss  0.6889 accur 0.794 | train loss  0.6401 accur 0.806 | valid loss  1.0005 accur 0.743\n",
      "Epoch:  10 accum loss  0.6616 accur 0.798 | train loss  0.6243 accur 0.807 | valid loss  0.9968 accur 0.736\n",
      "Epoch:  11 accum loss  0.6635 accur 0.795 | train loss  0.6961 accur 0.789 | valid loss  1.0596 accur 0.731\n",
      "Epoch:  12 accum loss  0.6376 accur 0.805 | train loss  0.6001 accur 0.824 | valid loss  0.9913 accur 0.748\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 979 ms, total: 1min 47s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.005, weight_decay=0.0)\n",
    "\n",
    "history = []\n",
    "epochs = 12\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        for i in range(p, p+batch_size): # do one batch\n",
    "            x = X_train[i]\n",
    "            h = torch.zeros(nhidden, 1, dtype=torch.float64, requires_grad=False)  # reset hidden state at start of record\n",
    "            for j in range(len(x)):  # for each char in a name\n",
    "                h = W.mm(h) + U.mm(onehot(x[j]))\n",
    "                h = torch.tanh(h)\n",
    "            # h is output of RNN, a fancy CBOW embedding for variable-length sequence in x\n",
    "            # run through a final layer to map that h to a one-hot encoded predicted class\n",
    "#             h = dropout(h, p=0.3)\n",
    "            o = V.mm(h)\n",
    "            o = o.reshape(1,nclasses)\n",
    "            o = softmax(o)\n",
    "            loss += cross_entropy(o, y_train[i])\n",
    "            correct = torch.argmax(o[0])==y_train[i]\n",
    "            epoch_training_accur += correct\n",
    "\n",
    "        # update matrices based upon loss computed from a batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "#         print(loss.detach().item())\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    epoch_training_loss /= n\n",
    "    epoch_training_accur /= n\n",
    "#     print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        o = forward(X_train)#, apply_softmax=False)\n",
    "        train_loss = cross_entropy(o, y_train)\n",
    "        correct = torch.argmax(o, dim=1).detach()==y_train\n",
    "        train_accur = torch.sum(correct) / float(len(X_train))\n",
    "\n",
    "        o = forward(X_valid)\n",
    "        valid_loss = cross_entropy(o, y_valid)\n",
    "        correct = torch.argmax(o, dim=1).detach()==y_valid\n",
    "        valid_accur = torch.sum(correct) / float(len(X_valid))\n",
    "\n",
    "        history.append((train_loss, valid_loss))\n",
    "        print(f\"Epoch: {epoch:3d} accum loss {epoch_training_loss:7.4f} accur {epoch_training_accur:4.3f} | train loss {train_loss:7.4f} accur {train_accur:4.3f} | valid loss {valid_loss:7.4f} accur {valid_accur:4.3f}\")\n",
    "\n",
    "history = torch.tensor(history)\n",
    "plot_history(history, yrange=(0,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing on 80% training from full data set:\n",
    "\n",
    "```\n",
    "CPU times: user 1min 46s, sys: 979 ms, total: 1min 47s\n",
    "Wall time: 1min 48s\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
