<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>How to explain gradient boosting</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="How to explain gradient boosting"/>
<meta property='og:image' content="http://explained.ai/gradient-boosting/images/golf-MSE.png">
<meta property='og:description' content="3-part article on how gradient boosting works for squared error, absolute error, and general loss functions. Deeply explained, but as simply and intuitively as possible."/>
<meta property='og:url' content="http://explained.ai/gradient-boosting/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="website" />

<!-- Twitter meta -->
<meta name="twitter:title" content="How to explain gradient boosting">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="3-part article on how gradient boosting works for squared error, absolute error, and general loss functions. Deeply explained, but as simply and intuitively as possible.">
<meta name="twitter:image" content="http://explained.ai/gradient-boosting/images/golf-MSE.png">
<!-- END META -->
</head>
<body>
<h1>How to explain gradient boosting</h1>

<div class="watermark">
<i>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<p><a href="http://parrt.cs.usfca.edu">Terence Parr</a> and <a href="http://www.fast.ai/about/#jeremy">Jeremy
	Howard</a></p>

<p style="font-size: 85%; line-height: 1.3em;">(We teach in University of San Francisco's <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">MS in Data Science program</a> and have other nefarious projects underway. You might know Terence as the creator of the <a href="http://www.antlr.org">ANTLR parser generator</a>. Jeremy is a founding researcher at <a href="http://fast.ai">fast.ai</a>, a research institute dedicated to making deep learning more accessible.)</font></p>
</p>

<p style="font-size: 85%; line-height: 1.3em;">Please send comments, suggestions, or fixes to <a href="mailto:parrt@cs.usfca.edu">Terence</a>.</p>

<div id="toc">
<p class="toc_title">Contents</p>
<ul>
	<li><a href="#overview">Roadmap</a>
	<li><a href="L2-loss.html">Distance to target</a>
		<ul>
			<li><a href="L2-loss.html#sec:2.1">An introduction to additive modeling</a>
			</li>
			<li><a href="L2-loss.html#sec:2.2">An introduction to boosted regression</a>
			</li>
			<li><a href="L2-loss.html#sec:2.3">The intuition behind gradient boosting</a>
			</li>
			<li><a href="L2-loss.html#sec:2.4">Gradient boosting regression by example</a>
			</li>
			<li><a href="L2-loss.html#sec:2.5">Measuring model performance</a>
			</li>
			<li><a href="L2-loss.html#sec:2.6">Choosing hyper-parameters</a>
			</li>
			<li><a href="L2-loss.html#alg:l2">GBM algorithm to minimize L2 loss</a>
		</ul>
		<li><a href="L1-loss.html">Heading in the right direction</a>
			<ul>
			<li><a href="L1-loss.html#sec:1.1">Chasing the sign vector</a>
			</li>
			<li><a href="L1-loss.html#sec:1.2">Two perspectives on training weak models for L1 loss</a>
			</li>
			<li><a href="L1-loss.html#sec:1.3">GBM optimizing MAE by example</a></li>
			<li><a href="L1-loss.html#sec:1.4">Comparing GBM trees that optimize MSE and MAE </a></li>
			
			<li><a href="L1-loss.html#alg:l1">GBM algorithm to minimize L1 loss</a>
	</ul>
	<li><a href="descent.html">Gradient boosting performs gradient descent</a>
	<ul>
		<li><a href="descent.html#sec:3.1">The intuition behind gradient descent</a>
		</li>
		<li><a href="descent.html#sec:3.2">Boosting as gradient descent in prediction space</a>
		<ul>
				<li><a href="descent.html#sec:3.2.1">The MSE function gradient</a></li>
				<li><a href="descent.html#sec:3.2.2">The MAE function gradient</a></li>
				<li><a href="descent.html#sec:3.2.3">Morphing GBM into gradient descent</a></li>
				<li><a href="descent.html#sec:3.2.4">Function space is prediction space</a></li>
		</ul>
		<li><a href="descent.html#sec:3.3">How gradient boosting differs from gradient descent</a>
		</li>
		<li><a href="descent.html#sec:3.4">Summary</a>
		</li>
		<li><a href="descent.html#sec:3.5">General algorithm with regression tree weak models</a>
	</ul>
	<li><a href="faq.html">Frequently asked questions (FAQ)</a>
	</ul>
</div>

<h3 id="overview">Roadmap</h3>

<p><img style="float:right;margin:0px 10px 0px 0;" width=205 src="images/golf-MSE.png"><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient boosting machines</a> (GBMs) are currently very popular and so it's a good idea for machine learning practitioners to understand how GBMs work. The problem is that understanding all of the mathematical machinery is tricky and, unfortunately, these details are needed to tune the hyper-parameters. (Tuning the hyper-parameters is required to get a decent GBM model unlike, say, Random Forests.) Our goal in this article is to explain the intuition behind gradient boosting, provide visualizations for model construction, explain the mathematics as simply as possible, and answer thorny questions such as why GBM is performing &ldquo;gradient descent in function space.&rdquo; We've split the discussion into three morsels and a FAQ for easier digestion.
</p>

<ol>

<li><a href="L2-loss.html">Gradient boosting: Distance to target</a>. <br>
First, we examine the most common form of GBM that optimizes the mean squared error (MSE), also called the <i>L</i><sub>2</sub> <i>loss</i> or <i>cost</i>. (The mean squared error is the average of the square of the difference between the true targets and the predicted values from a set of observations, such as a training or validation set.)  As we'll see, A GBM is a composite model that combines the efforts of multiple weak models to create a strong model, and each additional weak model reduces the mean squared error (MSE) of the overall model.  We give a fully-worked GBM example for a simple data set, complete with computations and model visualizations.<br><br>

<li><a href="L1-loss.html">Gradient boosting: Heading in the right direction</a>.<br>
Optimizing a model according to MSE makes it chase outliers because squaring the difference between targets and predicted values emphasizes extreme values. When we can't remove outliers, it's better to optimize the mean absolute error (MAE), also called the <i>L</i><sub>1</sub> <i>loss</i> or <i>cost</i>. (The MAE is the average of the absolute value of the difference between the true targets and the predicted values.) This second article shows the computations and visualizations for a GBM that optimizes MAE for the same data set as used in the first article to optimize MSE.<br><br>

<li><a href="descent.html">Gradient boosting performs gradient descent</a>.<br>
The previous two articles give the intuition behind GBM and the simple formulas to show how weak models join forces to create a strong regression model.  No attempt was made to show how we can abstract out a generalized GBM that works for any loss function. This last article demonstrates that gradient boosting is really doing a form of gradient descent and, therefore, is in fact optimizing MSE or MAE depending on the direction vectors we use to train the weak models. The discussion relies on a bit of derivative calculus but it's an important read if you'd like to learn deeply how GBM works. (To brush up on your vectors and derivatives, you can check out <a href="http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html">The Matrix Calculus You Need For Deep Learning</a>.) We finish off by clearing up a number of confusion points regarding gradient boosting.

<p>As Ben Gorman points out in <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">A Kaggle Master Explains Gradient Boosting</a>, &ldquo;<i>This is the part that gets butchered by a lot of gradient boosting explanations</i>.&rdquo; His blog post does a good job of explaining it, but we give our own perspective here.  

<li><a href="faq.html">Frequently asked questions (FAQ)</a>.<br>
There are lots of confusing things about gradient boosting machines and we have collected and answered a number of questions from students in this piece.
	
</ol>

<h3>Recommended reading</h3>

<p>To get started with GBM, those with very strong mathematical backgrounds can go directly to the super-tasty 1999 paper by Jerome Friedman called <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine</a>.  To get the practical and implementation perspective, though, we recommend Ben Gorman's excellent blog <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">A Kaggle Master Explains Gradient Boosting</a> and Prince Grover's <a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">Gradient Boosting from scratch</a> (written by a <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">data science</a> graduate student at the University of San Francisco; we also thank Prince for reviewing this article.) To go beyond basic GBM, see  <a href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A Scalable Tree Boosting System</a> by Chen and Guestrin. To get really fancy, you can even add momentum to the gradient descent performed by boosting machines, as shown in the recent article: <a href="https://arxiv.org/abs/1803.02042">Accelerated Gradient Boosting</a>.</p> 

<h3>Python notebooks</h3>

<p>
All of the code used to generate the graphs and data in these articles is available in the <a href="https://github.com/parrt/ml-articles/tree/master/gradient-boosting/notebooks">Notebooks directory at github</a>.  Warning: the code is a just good enough to generate the graphs in these articles; the code is provided purely for transparency and completeness (i.e., we ain't proud of it).

</body>
</html>
